{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_data = pd.read_csv(\"/data1/yejb/prosit/figure3/supply_origin/fine-tuned.csv\")\n",
    "table_data = pd.read_csv(\n",
    "    \"/data1/yejb/prosit/figure3/supply_origin/fine-tuned-IAA-noIAA-0.01.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_data[table_data['SM_v2'].apply(lambda x: bool(x))]['SM_v2']\n",
    "# table_data[table_data['Sequence'].apply(lambda x: len(x) == 7)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_allele_len(data, allele, length):\n",
    "    alleles = data[data['Allele'] == allele]\n",
    "    alleles = alleles[alleles['Sequence'].apply(lambda x: len(x) == length)]\n",
    "    baseline = alleles[alleles['SM_v2'].apply(\n",
    "        lambda x: bool(x))]['Sequence'].to_list()\n",
    "    prosit = alleles[alleles['Prosit'].apply(\n",
    "        lambda x: bool(x))]['Sequence'].to_list()\n",
    "    finetuned = alleles[alleles['Fine-tuned Prosit'].apply(\n",
    "        lambda x: bool(x))]['Sequence'].to_list()\n",
    "    return baseline, prosit, finetuned\n",
    "\n",
    "def LSG(p1, p2):\n",
    "    p1 = set(p1)\n",
    "    p2 = set(p2)\n",
    "    return p1-p2, p1.intersection(p2), p2-p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_unique_char(data):\n",
    "    all_peptides = data[\"Sequence\"]\n",
    "    all_peptides = \"\".join(all_peptides)\n",
    "    return set(all_peptides)\n",
    "\n",
    "def extract_pos(peps, pos):\n",
    "    return [ [p[i] for i in pos]for p in peps]\n",
    "\n",
    "class Labeler:\n",
    "    def __init__(self, unique_char):\n",
    "        self._reverse = list(unique_char)\n",
    "        self._dict = {\n",
    "            c : i\n",
    "            for i, c in enumerate(self._reverse)\n",
    "        }\n",
    "    def encode(self, peptide):\n",
    "        return [self._dict[c] for c in peptide]\n",
    "\n",
    "    def decode(self, pep_index):\n",
    "        return \"\".join([self._reverse[i] for i in pep_index])\n",
    "    \n",
    "    def encode_batch(self, peptides):\n",
    "        return [self.encode(p) for p in peptides]\n",
    "    \n",
    "    def decode_batch(self, pep_indexs):\n",
    "        return [self.decode(p) for p in pep_indexs]\n",
    "    \n",
    "    def position_matrix(self, pep_indexs, length):\n",
    "        mat = np.zeros((length, len(self._reverse)))    \n",
    "        for p in pep_indexs:\n",
    "            for i in range(length):\n",
    "                mat[i, p[i]] += 1\n",
    "        return mat\n",
    "    \n",
    "    def emission_mat(self, pep_indexs, length):\n",
    "        mat = self.position_matrix(pep_indexs, length)\n",
    "        return mat/(mat.sum(1).reshape(-1, 1) + 1e-9)\n",
    "    \n",
    "    def get_top_pos(self, pep_indexs, length, topk=5):\n",
    "        mat = self.position_matrix(pep_indexs, length)\n",
    "        pos_rank = np.max(mat, axis=1)\n",
    "        return np.argsort(pos_rank)[::-1][:topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_alleles = table_data['Allele'].unique()\n",
    "all_length = [8, 9, 10, 11]\n",
    "\n",
    "topk_pos = 5\n",
    "labeller = Labeler(all_unique_char(table_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:54<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "def emission_prob(em_mat, pep_index):\n",
    "    assert len(em_mat) == len(pep_index)\n",
    "    prob = 0\n",
    "    for i in range(len(em_mat)):\n",
    "        prob += np.log10(1e-7 + em_mat[i, pep_index[i]])\n",
    "    return prob\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "prosit_dict = defaultdict(list)\n",
    "finetuned_dict = defaultdict(list)\n",
    "\n",
    "all_length = [8, 9, 10, 11]\n",
    "for allele in tqdm(all_alleles):\n",
    "    for length in all_length:\n",
    "        baseline, prosit, finetuned = read_allele_len(\n",
    "            table_data, allele, length)\n",
    "        if len(baseline) == 0:\n",
    "            continue\n",
    "        baseline_i = labeller.encode_batch(set(baseline + prosit + finetuned))\n",
    "        needed_pos = labeller.get_top_pos(baseline_i, length, topk=topk_pos)\n",
    "\n",
    "        inter = extract_pos(baseline_i, needed_pos)\n",
    "        em_mat = labeller.emission_mat(inter, topk_pos)\n",
    "        # prosit\n",
    "        loss, shared, gain = LSG(baseline, prosit)\n",
    "        for name, pep in zip(['Loss', 'Shared', \"Gain\"], [loss, shared, gain]):\n",
    "            ems = []\n",
    "            pep_i = labeller.encode_batch(pep)\n",
    "            if len(pep_i) == 0:\n",
    "                continue\n",
    "            pep_i = np.array(extract_pos(pep_i, needed_pos))\n",
    "            for p in pep_i:\n",
    "                s = emission_prob(em_mat, p)\n",
    "                ems.append(s)\n",
    "            prosit_dict[name].extend(ems)\n",
    "        # finetuned\n",
    "        loss, shared, gain = LSG(baseline, finetuned)\n",
    "        for name, pep in zip(['Loss', 'Shared', \"Gain\"], [loss, shared, gain]):\n",
    "            ems = []\n",
    "            pep_i = labeller.encode_batch(pep)\n",
    "            if len(pep_i) == 0:\n",
    "                continue\n",
    "            pep_i = np.array(extract_pos(pep_i, needed_pos))\n",
    "            for p in pep_i:\n",
    "                s = emission_prob(em_mat, p)\n",
    "                ems.append(s)\n",
    "            finetuned_dict[name].extend(ems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Mean: -4.735 Len: 17490\n",
      "Shared Mean: -4.284 Len: 159302\n",
      "Gain Mean: -4.790 Len: 78742\n"
     ]
    }
   ],
   "source": [
    "for k, v in prosit_dict.items():\n",
    "    print(k, f\"Mean: {np.mean(v):.3f}\", f\"Len: {len(v):3d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Mean: -4.945 Std: 13649.000\n",
      "Shared Mean: -4.277 Std: 163143.000\n",
      "Gain Mean: -6.062 Std: 144740.000\n"
     ]
    }
   ],
   "source": [
    "for k, v in finetuned_dict.items():\n",
    "    print(k, f\"Mean: {np.mean(v):.3f}\", f\"Std: {len(v):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "prosit_dict = defaultdict(list)\n",
    "finetuned_dict = defaultdict(list)\n",
    "\n",
    "all_length = [8]\n",
    "for allele in tqdm(all_alleles):\n",
    "    for length in all_length:\n",
    "        baseline, prosit, finetuned = read_allele_len(\n",
    "            table_data, allele, length)\n",
    "\n",
    "        baseline_i = labeller.encode_batch(baseline)\n",
    "        needed_pos = labeller.get_top_pos(baseline_i, length, topk=topk_pos)\n",
    "\n",
    "        inter = extract_pos(baseline_i, needed_pos)\n",
    "        baseline_train = np.array(inter)\n",
    "\n",
    "        hmm_length = [topk_pos] * len(baseline_train)\n",
    "        baseline_train = np.concatenate(baseline_train).reshape(-1, 1)\n",
    "        hmm_model = hmm.MultinomialHMM(n_components=3, n_iter=50)\n",
    "        hmm_model.n_features = len(labeller._reverse)\n",
    "        remodel = hmm_model.fit(baseline_train, hmm_length)\n",
    "\n",
    "        # prosit\n",
    "        loss, shared, gain = LSG(baseline, prosit)\n",
    "        for name, pep in zip(['Loss', 'Shared', \"Gain\"], [loss, shared, gain]):\n",
    "            ems = []\n",
    "            pep_i = labeller.encode_batch(pep)\n",
    "            pep_i = np.array(extract_pos(pep_i, needed_pos))\n",
    "            pep_length = [topk_pos] * len(pep_i)\n",
    "            if len(pep_i) == 0:\n",
    "                continue\n",
    "            # pep_i = np.concatenate(pep_i).reshape(-1, 1)\n",
    "            for p in pep_i:\n",
    "                s = remodel.score(p.reshape(-1, 1))\n",
    "                if np.isinf(s):\n",
    "                    continue\n",
    "                ems.append(s)\n",
    "            prosit_dict[name].append(np.mean(ems))\n",
    "        # finetuned\n",
    "        loss, shared, gain = LSG(baseline, finetuned)\n",
    "        for name, pep in zip(['Loss', 'Shared', \"Gain\"], [loss, shared, gain]):\n",
    "            ems = []\n",
    "            pep_i = labeller.encode_batch(pep)\n",
    "            pep_i = np.array(extract_pos(pep_i, needed_pos))\n",
    "            if len(pep_i) == 0:\n",
    "                continue\n",
    "            for p in pep_i:\n",
    "                s = remodel.score(p.reshape(-1, 1))\n",
    "                if np.isinf(s):\n",
    "                    continue\n",
    "                ems.append(s)\n",
    "            finetuned_dict[name].append(np.mean(ems))\n",
    "        # prosit_i = labeller.encode_batch(prosit)\n",
    "        # finetuned_i = labeller.encode_batch(finetuned)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17465660725f2b6bdd0101ccb038f8fe4c98405cd7a343be234717463386f391"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
